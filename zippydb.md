#### [ZippyDB](https://engineering.fb.com/2021/08/06/core-data/zippydb/)
1. ZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. 
2. ZippyDB offers a lot of flexibility to applications in terms of tunable durability, consistency, availability, and latency guarantees, which has made the service a popular choice within Facebook
3. Consequently, most of our initial efforts were focused on building a reusable and flexible data replication library called Data Shuttle. 
4. We built a fully managed distributed key-value store by combining Data Shuttle with a preexisting and well-established storage engine (RocksDB) and layering this on top of our existing shard management (Shard Manager) and distributed configuration service (built on ZooKeeper), that together solves load balancing, shard placement, failure detection, and service discovery.
5. From educative
   1. ZippyDB enables the system to perform a large amount of write operations efficiently while providing good performance for the read workload.
   2. RocksDB is primarily an efficient storage engine that can be embedded in other applications as a library. It frees the programmers from the messy details of efficient storage and lets them concentrate on their specific problems. RocksDB is optimized for write-heavy workload by using log-structured storage. The read performance is good because of the use of Bloom filters.
   3. A snapshot read is an extremely cheap operation that uses snapshot handlers to perform multiple reads that are consistent with each other. When read or write operations occur simultaneously on the same file, a snapshot of that file is generated, and the client with a read request is propagated to the snapshot, as shown in the slides below. (In some sense, snapshots make a replica of data as needed and enable non-blocking reads when writes are in progress.) A new client will also go to the block, but when it will realize that the block is locked, the snapshot handler will redirect that client to the snapshot so that it can start reading.
   4. Reading or writing partial data can leave data inconsistent. We implement read-modify-write on the server side to avoid inconsistency in partial write operations. We can perform partial read operations by combining data of different blocks and trying to get most of the data from the same node.
   5. In ZippyDB, shards are the units on a tier containing data regarding a use case. On the server side, the shard is the basic unit of management, and each shard contains multiple replicas of data placed geographically for fault tolerance. Depending on the configuration, we can either use Paxos or async replication.
   6. Some of the shard replicas (possibly a remote one) that were left out from the synchronous replication are managed via asynchronous replication.
   7. Additional replicas can be generated to manage read load. Such replicas are called followers, and they donâ€™t participate in write operations. They just observe the current state of a shard and replicate it locally. We can use this strategy to bring in updates inside a data center, and then others can sync up locally from the followers without generating any traffic to original shard Paxos groups. Although followers can be behind the primary replica, these replicas can handle low-latency readings.
   8. Clients can use an appropriate combination of these synchronous and asynchronous replication schemes.They can also configure between the number of follower replicas and quorum size, where the quorum size tells us the number of followers that have to acknowledge the leader before the acknowledgement is sent to the client. It enables users to choose their ideal combination of consistency, durability, read performance, and write performance.
   9. ZippyDB divides time into small epochs. For each epoch, ZippyDB assigns a known primary replica for a shard. This primary replica is the Paxos leader for writing. All the writes come to the primary, which is then synchronously written to the majority quorum and asynchronously to some remote replicas. Such a mechanism provides strongly consistent write operations. Clients might opt for a looser consistency model if they need to.
   10. For reads, clients have three options. For consistent reading, they can go to the current primary of the shard. Clients can also go to some secondary replicas for reading if they can tolerate somewhat stale information. Such replicas eventually become consistent. ZippyDB actually provides stronger guarantees that a secondary replica will have a bounded staleness. Once that bound is reached for a secondary, it stops serving reads. Clients can also get read-your-writes consistency by using a special tag that was returned in the previous write. Now, when the client sends the read request with that tag/token to the secondary, it should get the updated data. If that secondary has not received those updates, the secondary will not return any data.
   11. Components
       1. Data shuttle: For reads, clients have three options. For consistent reading, they can go to the current primary of the shard. Clients can also go to some secondary replicas for reading if they can tolerate somewhat stale information. Such replicas eventually become consistent. ZippyDB actually provides stronger guarantees that a secondary replica will have a bounded staleness. Once that bound is reached for a secondary, it stops serving reads. Clients can also get read-your-writes consistency by using a special tag that was returned in the previous write. Now, when the client sends the read request with that tag/token to the secondary, it should get the updated data. If that secondary has not received those updates, the secondary will not return any data.
       2. Shard manager:
          1. Shard management not only manages the shard but also manages replication and performs load balancing. This system provides the optimal shard placement and management in addition to managing roles. It not only tells the data shuttle that it owns the shard but also tells the role of the shard. If a server goes offline, it has to move all the primary roles to another server. An automated load balancing technique ensures that workload is even on the entire tier.
          2. It's classic facebook shard manager.
       3. The request handler sends read requests directly to the store and sends write requests to the data shuttle, which does its replication. After writing the data, it returns the acknowledgment of request completion
       4. ZippyDB hosts the file system's metadata, and its durability and integrity are critical for Tectonic. Tectonic uses a separate instance of ZippyDB for security and performance-isolation reasons. This instance uses strongly consistent writes and reads. ZippyDB, an end-to-end managed service, can be considered a BlackBox that provides durable, scalable, and highly performant key-value services to Tectonic.
       5. 

